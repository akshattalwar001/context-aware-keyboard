{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "YfZtc9qyBekx"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import numpy as np\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/books_clean.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    combined = f.read()\n",
        "\n",
        "print(\"Loaded:\", len(combined), \"characters\")\n",
        "print(combined[:300])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a_yHa0iqCefd",
        "outputId": "943e496d-df7b-42af-fc02-0b19a913aee4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded: 3081162 characters\n",
            "PREFACE.\n",
            "Walt Whitman has somewhere a fine and just distinction between \"loving\n",
            "by allowance\" and \"loving with personal love.\" This distinction applies\n",
            "to books as well as to men and women; and in the case of the not very\n",
            "numerous authors who are the objects of the personal affection, it\n",
            "brings a cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = combined.lower()"
      ],
      "metadata": {
        "id": "mcUwAejPCxPJ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "VOCAB_SIZE = 5000\n",
        "\n",
        "tokenizer = Tokenizer(num_words=VOCAB_SIZE, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts([data])\n",
        "\n",
        "print(\"Unique words found:\", len(tokenizer.word_index))\n",
        "print(\"Vocab size used:\", VOCAB_SIZE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OzzI_nlXDXaH",
        "outputId": "199bc770-e80a-40cd-a088-2b83fd4468b6"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique words found: 19728\n",
            "Vocab size used: 5000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sequence = tokenizer.texts_to_sequences([data])[0]\n",
        "print(\"Total tokens:\", len(sequence))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NMyaBUo5Day3",
        "outputId": "b1c7b7e1-da7b-4538-eca1-0bc4c5703657"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total tokens: 564913\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SEQUENCE_LENGTH = 5\n",
        "\n",
        "input_sequences = []\n",
        "for i in range(SEQUENCE_LENGTH, len(sequence)):\n",
        "    input_sequences.append(sequence[i-SEQUENCE_LENGTH:i+1])\n",
        "\n",
        "input_sequences = np.array(input_sequences)\n",
        "\n",
        "np.random.shuffle(input_sequences)\n",
        "\n",
        "X = input_sequences[:, :-1]\n",
        "y = input_sequences[:, -1]\n",
        "\n",
        "print(\"X shape:\", X.shape)\n",
        "print(\"y shape:\", y.shape)\n",
        "print(\"Max y:\", y.max())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G3lSshLJDstA",
        "outputId": "6b92e0cc-04eb-4c4e-f229-dd445dd0e111"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X shape: (564908, 5)\n",
            "y shape: (564908,)\n",
            "Max y: 4999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential([\n",
        "    tf.keras.layers.Input(shape=(SEQUENCE_LENGTH,)),\n",
        "    Embedding(input_dim=VOCAB_SIZE, output_dim=128),\n",
        "    LSTM(150, return_sequences=True, dropout=0.2, recurrent_dropout=0.2),\n",
        "    LSTM(100, dropout=0.2, recurrent_dropout=0.2),\n",
        "    Dense(VOCAB_SIZE, activation='softmax')\n",
        "])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        },
        "id": "xEIeHuy5D0WT",
        "outputId": "c189f178-0a68-489c-dc93-74283a89dc54"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │       \u001b[38;5;34m640,000\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m150\u001b[0m)         │       \u001b[38;5;34m167,400\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │       \u001b[38;5;34m100,400\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m)           │       \u001b[38;5;34m505,000\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │       <span style=\"color: #00af00; text-decoration-color: #00af00\">640,000</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>)         │       <span style=\"color: #00af00; text-decoration-color: #00af00\">167,400</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">100,400</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>)           │       <span style=\"color: #00af00; text-decoration-color: #00af00\">505,000</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,412,800\u001b[0m (5.39 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,412,800</span> (5.39 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,412,800\u001b[0m (5.39 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,412,800</span> (5.39 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    optimizer=Adam(learning_rate=5e-4, clipnorm=1.0),\n",
        "    metrics=['accuracy']\n",
        ")"
      ],
      "metadata": {
        "id": "WYJaewAPD_GY"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    X, y,\n",
        "    epochs=10,\n",
        "    batch_size=256,\n",
        "    validation_split=0.1\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "lr4PNzGuEG7C",
        "outputId": "f9e31ba7-d583-4967-f5cc-e248155901bf"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m1987/1987\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 46ms/step - accuracy: 0.0621 - loss: 6.3809 - val_accuracy: 0.0954 - val_loss: 5.8423\n",
            "Epoch 2/10\n",
            "\u001b[1m1987/1987\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 47ms/step - accuracy: 0.1009 - loss: 5.7686 - val_accuracy: 0.1179 - val_loss: 5.5822\n",
            "Epoch 3/10\n",
            "\u001b[1m1987/1987\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 46ms/step - accuracy: 0.1232 - loss: 5.5280 - val_accuracy: 0.1305 - val_loss: 5.4337\n",
            "Epoch 4/10\n",
            "\u001b[1m1987/1987\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 47ms/step - accuracy: 0.1325 - loss: 5.3819 - val_accuracy: 0.1369 - val_loss: 5.3150\n",
            "Epoch 5/10\n",
            "\u001b[1m1987/1987\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 48ms/step - accuracy: 0.1391 - loss: 5.2575 - val_accuracy: 0.1409 - val_loss: 5.2335\n",
            "Epoch 6/10\n",
            "\u001b[1m1987/1987\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 46ms/step - accuracy: 0.1434 - loss: 5.1613 - val_accuracy: 0.1445 - val_loss: 5.1715\n",
            "Epoch 7/10\n",
            "\u001b[1m1987/1987\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 46ms/step - accuracy: 0.1468 - loss: 5.0961 - val_accuracy: 0.1470 - val_loss: 5.1249\n",
            "Epoch 8/10\n",
            "\u001b[1m1987/1987\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 46ms/step - accuracy: 0.1497 - loss: 5.0388 - val_accuracy: 0.1495 - val_loss: 5.0905\n",
            "Epoch 9/10\n",
            "\u001b[1m1987/1987\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 45ms/step - accuracy: 0.1534 - loss: 4.9809 - val_accuracy: 0.1530 - val_loss: 5.0627\n",
            "Epoch 10/10\n",
            "\u001b[1m1987/1987\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 46ms/step - accuracy: 0.1564 - loss: 4.9413 - val_accuracy: 0.1555 - val_loss: 5.0360\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "SEQUENCE_LENGTH = 5\n",
        "\n",
        "def predict_next(seed_text, top_n=3):\n",
        "    seed_text = seed_text.lower()\n",
        "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "    token_list = token_list[-SEQUENCE_LENGTH:]\n",
        "    token_list = pad_sequences([token_list], maxlen=SEQUENCE_LENGTH, padding='pre')\n",
        "\n",
        "    predictions = model.predict(token_list, verbose=0)[0]\n",
        "    top_indices = predictions.argsort()[::-1]\n",
        "\n",
        "    suggestions = []\n",
        "    for idx in top_indices:\n",
        "        word = tokenizer.index_word.get(idx, \"\")\n",
        "        if word and word != \"<OOV>\":\n",
        "            suggestions.append(word)\n",
        "        if len(suggestions) == top_n:\n",
        "            break\n",
        "\n",
        "    return suggestions\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"Enter text (or 'quit' to stop): \")\n",
        "    if user_input.lower() == 'quit':\n",
        "        break\n",
        "    predictions = predict_next(user_input)\n",
        "    print(\"Top 3 predictions:\", predictions)\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0xijyrf5EKkl",
        "outputId": "3f702bef-4c61-4199-bc15-a835e6fba856"
      },
      "execution_count": 18,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter text (or 'quit' to stop): hello\n",
            "Top 3 predictions: ['and', 'in', 'the']\n",
            "\n",
            "Enter text (or 'quit' to stop): i want to know the only \n",
            "Top 3 predictions: ['time', 'thing', 'day']\n",
            "\n",
            "Enter text (or 'quit' to stop): he exactly told me good\n",
            "Top 3 predictions: ['i', 'that', 'gray']\n",
            "\n",
            "Enter text (or 'quit' to stop): it was a dark\n",
            "Top 3 predictions: ['man', 'and', 'thing']\n",
            "\n",
            "Enter text (or 'quit' to stop): the old man\n",
            "Top 3 predictions: ['was', 'had', 'who']\n",
            "\n",
            "Enter text (or 'quit' to stop): quit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_sentence(seed_text, total_words=10):\n",
        "    result = seed_text\n",
        "\n",
        "    for _ in range(total_words):\n",
        "        token_list = tokenizer.texts_to_sequences([result.lower()])[0]\n",
        "        token_list = token_list[-SEQUENCE_LENGTH:]\n",
        "        token_list = pad_sequences([token_list], maxlen=SEQUENCE_LENGTH, padding='pre')\n",
        "\n",
        "        predictions = model.predict(token_list, verbose=0)[0]\n",
        "        top_indices = predictions.argsort()[::-1]\n",
        "\n",
        "        next_word = \"\"\n",
        "        for idx in top_indices:\n",
        "            word = tokenizer.index_word.get(idx, \"\")\n",
        "            if word and word != \"<OOV>\":\n",
        "                next_word = word\n",
        "                break\n",
        "\n",
        "        result = result + \" \" + next_word\n",
        "\n",
        "    return result\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"Enter seed text (or 'quit' to stop): \")\n",
        "    if user_input.lower() == 'quit':\n",
        "        break\n",
        "    sentence = generate_sentence(user_input)\n",
        "    print(\"Generated:\", sentence)\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XY_xpSaMI5Z2",
        "outputId": "0a9d81e7-9dfb-43d2-e77e-ae069a7bbf99"
      },
      "execution_count": 19,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter seed text (or 'quit' to stop): hey there i just wanted to tell you\n",
            "Generated: hey there i just wanted to tell you to be a very good thing to be a little\n",
            "\n",
            "Enter seed text (or 'quit' to stop): he was a very\n",
            "Generated: he was a very little man in the room and the whole man was\n",
            "\n",
            "Enter seed text (or 'quit' to stop): man\n",
            "Generated: man and i am sure to be a little thing to\n",
            "\n",
            "Enter seed text (or 'quit' to stop): hello there i wish you\n",
            "Generated: hello there i wish you are not a little thing to be a little thing\n",
            "\n",
            "Enter seed text (or 'quit' to stop): quit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "\n",
        "model.save(\"/content/next_word_model.keras\")\n",
        "\n",
        "with open(\"/content/tokenizer.pkl\", \"wb\") as f:\n",
        "    pickle.dump(tokenizer, f)\n",
        "\n",
        "print(\"Model and tokenizer saved!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uawxEPPKJbqL",
        "outputId": "ed19da7a-4cab-4053-e704-ed2b75785c83"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model and tokenizer saved!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "K9gI5voMJ_4P"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}